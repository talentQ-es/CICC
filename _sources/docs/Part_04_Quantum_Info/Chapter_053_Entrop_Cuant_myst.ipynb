{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710bfdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> {sub-ref}`today` | {sub-ref}`wordcount-minutes` min read\n",
    "\n",
    "::::{figure} ../figuras/logos/Logo_TalentQ_Azul.png\n",
    ":width: 150px\n",
    ":align: right\n",
    "::::\n",
    "\n",
    "\n",
    "# Elementos de Información Cuántica\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\ketbra}[2]{| #1\\rangle \\langle #2|} $\n",
    "$ \\newcommand{\\tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\Tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\cal H}} $\n",
    "$ \\newcommand{\\V}{{\\cal V}} $\n",
    "$ \\newcommand{\\Lin}{\\hbox{Lin}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608d032",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Entropías de información clásicas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099a90d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "por completitud, vamos a repasar los aspectos fundamentales de la teoría clásica de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49309922",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ent_Shannon'></a>\n",
    "### Entropía de Shannon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b1293",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Definición</b>: </i>entropía de Shannon</i> \n",
    "\n",
    "^^^\n",
    "\n",
    "La entropía de Shannon asociada a una variabla aleatoria clásica $X = (x,p(x))$ viene dada por la expresión\n",
    "\n",
    "    \n",
    "$$\n",
    "H(X) = -\\sum_{x} p(x) \\log p(x)\n",
    "$$  \n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3901c0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "La entropía de Shannon es una medida de la  *incertidumbre* que elimina, en promedio, el resultado $x\\in X$  de una *tirada*. Por tanto, es una medida también de la cantidad de información que, en promedio, alberga cada suceso de la variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ebe85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La entropía de Shannon posee las siguientes propiedades\n",
    "\n",
    "- sea  $N$  el número de elementos de $X$, se cumple $ 0 \\leq H(X) \\leq \\log N$ \n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d1dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- $H(X)$ es una función *cóncava* de $X$. Para $\\lambda \\in (0,1)$ y $X$, $Y$ dos variables aleatorias\n",
    "\n",
    "$$\n",
    "H\\left( \\rule{0mm}{4mm} \\lambda X + (1-\\lambda) Y)\\right) ~ \\geq ~ \\lambda H(X) + (1-\\lambda) H(Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced4a2",
   "metadata": {},
   "source": [
    "Esto quiere decir que hay **más incertidumbre** en el colectivo de eventos conjuntos $xy\\in XY$ que la **suma de las incertidumbres** de cada una de las variables estocásticas por separado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7aa92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{admonition} Ejemplo\n",
    ":class: tip\n",
    "\n",
    "\n",
    "La entropía de la distribución binaria (o de Bernoulli) $ \\{ (a,b),(p,(1-p) \\} $ es \n",
    "\n",
    "\n",
    "    \n",
    "$$\n",
    "H(p) = - p\\log p -(1-p)\\log (1-p)\n",
    "$$\n",
    "\n",
    "\n",
    " \n",
    "::::{figure} figuras/binaryentropy.png\n",
    ":width: 400px\n",
    ":align: center\n",
    "::::\n",
    "    \n",
    "\n",
    "El caso $p=1/2$ marca el máximo de la entropía. En este valor es donde, en promedio, conocer el resultado $a$ ó $b$ de una tirada, remueve la mayor cantidad de incertidumbre. El valor $H(1/2) = 1$ define la unidad de información cuando usamos logaritmos en base 2, y recibe el nombre 1 bit de información.\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123d137",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Otra interpretación de $H(X)$ proviene de la cantidad de recursos que necesitamos para transmitir mensajes formados con *letras* de la variabla aleatoria $X$. Esto es el contenido del primer teorema de Shannon.\n",
    "\n",
    "Para transmitir un mensaje a través de un canal se recurre a un proceso de codificación comprimida. Generalmente se comprime en bits $\\{0,1\\}$ pero el uso de dits $\\{0,...,D-1\\}$ es posible.\n",
    "\n",
    "De hecho, asociado a una codificación en dits, se define $H_D(X) = -\\sum_{x} p(x) \\log_D p(x)$. Cuando no mencionamos $D$, estaremos suponiendo que $D=2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb076458",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>: </i>Primer Teorema de Shannon, de Codificación de Fuentes</i> \n",
    "\n",
    "^^^\n",
    "\n",
    " Sea $X = \\{a_i,p(a_i)\\}$ una fuente estocastica de $N$ letras. En el límite $n\\to \\infty$ es posible <b>codificar sin pérdidas</b> palabras  de $n$ letras  utilizando en promedio  $H(X)$ bits por cada letra de fuente .  \n",
    "\n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b686a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "El truco que se le ocurrión a Shannon  es renunciar a codificar las letras de la fuente de forma individual, y a cambio, codificar grupos de letras de longitud $n$.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dad23",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "La clave de este teorema reside la caracterización del denominado *conjunto típico* de $n-$cadenas $T_n = \\{\\bar x^n\\}$. En el límite $n\\to \\infty$ se demuestran las siguientes tres propiedades\n",
    "\n",
    "- cualquier secuencia que no esté en el conjunto típico tiene probabilidad nula de aparecer\n",
    "\n",
    "\n",
    "- el número de secuencias en el conjunto típico es $|T| ~ \\stackrel{n\\to\\infty}{\\longrightarrow} ~ 2^{n H(X)}$.\n",
    "\n",
    "- cualquier secuencia que esté en el conjunto típico aparece con probabilidad uniforme (necesariamente $p(\\bar x^k) \\stackrel{n\\to\\infty}{\\longrightarrow} |T|^{-1} =  2^{- nH(X)}$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e6032",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Para probar estas tres propiedades hay que realizar un análisis de la probabilidad de ocurrencia de un mensaje de $n$ letras escogidas de un alfabeto de $N$ símbolos\n",
    "\n",
    "$$\n",
    "x^{n} \\sim \\underbrace{a_1\\cdots a_1}_{N(a_1|x^n)} \\underbrace{a_2\\cdots a_2}_{N(a_2|x^n)} \\cdots \\underbrace{a_N\\cdots a_N}_{N(a_N|x^n)} \\label{repclase}\n",
    "$$\n",
    "\n",
    "donde $N(a_i|x^n)$ es el número de veces que aparece la letra $a_i$ en el mensaje $x^n$. El número de $n-$cadenas posibles es $N^n = 2^{n\\log N}$.  \n",
    "\n",
    "\n",
    "Bajo la hipótesis de <i>independencia  e igualdad estadística</i> la probabilidad de una secuencia concreta será\n",
    "\n",
    "$$\n",
    "p (x^n) ~=~ \\prod_{i=1}^n p(x_i) ~=~  \\prod_{i=1}^N p(a_i)^{N(a_i|x^n)}\\, .\\label{pronseq}\n",
    "$$\n",
    "\n",
    "se demuestra que cuando $n\\to \\infty$ <u>prácticamente todas las secuencias</u> $x^n$ que se generan aleatoriamente pertenecen al *conjunto típico* $\\bar x^n$, caracterizado por \n",
    "\n",
    "$$\n",
    "\\left|-\\frac{1}{n} \\log \\, p(\\bar x^n) - H(X)\\right| \\leq \\delta\n",
    "$$\n",
    "\n",
    "El número de secuencias típicas es <i>subexponencial</i> con respecto al conjunto de secuencias posible\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{[\\bar x^n ]}{N^n} =  \\frac{n!}{\\prod_{i=1}^N (N(a_i | \\bar x^n)!}\\frac{1}{N^n} &\\stackrel{n\\to\\infty}{\\to}& \\frac{n!}{\\prod_{i=1}^N (n p_X(a_i))!}  \\frac{1}{N^n} = 2^{\\log n! - \\sum_{i=1}^N \\log (n p_X(a_i))!-n\\log N} \\nonumber\\\\\n",
    "\\rule{0mm}{7mm}&\\sim & 2^{n\\log n - n/\\ln 2 - \\sum_i (np_X(a_i) \\log n p_X(a_i)+ n p_X(a_i) /\\ln 2)-n\\log N}  \\nonumber\\\\\n",
    "& \\sim  & 2^{-n (\\log N- H_X)}  \n",
    "\\rule{0mm}{7mm}\n",
    "\\end{eqnarray}\n",
    "\n",
    " Finalmente, la probabilidad de encontrar\n",
    "una secuencia típica  $\\bar x^n = x_1...x_n$, será \n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    " p(\\bar x^n) &=& p(x_1)p(x_2)....p(x_n)  \\nonumber\\\\\n",
    "\\rule{0mm}{5mm} &\\simeq & p(a_1)^{n p(a_1)} ...p(a_N)^{np(a_N)} \\nonumber\\\\\n",
    "\\rule{0mm}{5mm} &=& \n",
    "2^{np(x_1) \\log p(a_1) + ... + np(a_N)\\log p(a_N)} = 2^{-n H_X}\n",
    "\\end{eqnarray}\n",
    "\n",
    "independiente de la secuencia concreta. En otras palabras,  las secuencias típicas son equiprobables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87d5d1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Las anteriores observaciones permiten establecer un procedimiento de codificación de mensajes: sólo tiene sentido codificar llos  $|T| = 2^{nH(X)}$ mensajes típicos que pueden aparecer. \n",
    "\n",
    "Esto se puede realizar con cadenas que tengan  $n H(X)$ bits por mensaje, o $H(X)$ bits por letra. \n",
    "\n",
    "Si $X$ es un alfabeto equiprobable, entonces $H(X) = \\log N$ y no es posible comprimirlo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1fa8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{admonition} Notar\n",
    ":class: note\n",
    "\n",
    "\n",
    "La cantidad $~ \\log N - H(X) ~$ cuantifica la <i>compresibilidad  </i> en una fuente aleatoria $X$   \n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f996810",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Entropías combinadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f525326",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "En un proceso de comunicación hay dos variables aleatorias, la emisión $X = (x, p(x))$ y la recepción $Y = (y, p(y))$. \n",
    "\n",
    "La ocurrencia conjunta de una pareja de valores $x$ e $y$ en los extremos del canal define una nueva variable aleatoria $XY = \\{xy, p(x,y)\\}$ conde  $p(x,y)$ es la *probabilidad combinada*.\n",
    "\n",
    "Como cualquier variable aleatoria, podemos también asociarle una entropía\n",
    "\n",
    "$$\n",
    "H(X,Y) = - \\sum_{xy} p(x,y) \\log p(x,y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81cf3a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Entropía condicional\n",
    "\n",
    "La probabilidad condicional es la herramienta matemática que caracteriza el ruido en un canal de comunicación.\n",
    "Concretamente\n",
    "la matriz $Q_{xy} = p(x|y)$ indica la probabilidad de que, habiendo recibido $y$, la letra emitida haya sido $x$. Un canal sin ruido debería tener $Q_{xy} = \\delta_{xy}$.\n",
    "\n",
    "En un canal con ruido, la variable aleatoria $X|y = \\{x, p(x|y)\\}$ informa de la probabilidad de obtener cualquier $x$ cuando por el otro lado $y$ fue enviado. \n",
    "\n",
    "Como a cualquier variable aleatoria, podemos asignarle una entropía \n",
    "\n",
    "$$\n",
    "H(X|y) = -\\sum_x p(x|y)\\log(p(x|y)).\n",
    "$$\n",
    "\n",
    "Si ahora promediamos $H(X|y)$ sobre los posibles mensajes $y$ enviados, obtendremos la entropía condicional\n",
    "\n",
    "\n",
    "$$\n",
    "H(X|Y) = \\sum_y p(y)H(X|y) = H(X,Y)  - H(Y)\n",
    "$$\n",
    ":::{dropdown} Demostración\n",
    "\\begin{eqnarray*}\n",
    "\\sum_y p(y)H(X|y) &=& -\\sum_{yx} p(y)p(x|y)\\log(p(x|y)) \\\\\n",
    "   &=& -\\sum_{yx} p(x,y)\\log(p(x,y)/p(y)) = -\\sum_{yx} p(x,y)\\log(p(x,y) + \\sum_{yx} p(x,y)\\log(p(y))\n",
    "    \\\\ &=&  = H(X,Y) + \\sum_{y} p(y)\\log(p(y)) =  H(X,Y)  - H(Y)\n",
    "\\end{eqnarray*}\n",
    ":::\n",
    "\n",
    "\n",
    "y refleja la incertidumbre residual que aun queda en $X$ después de haber recibido una gran cantidad de mensajes de $y \\in Y$. \n",
    "\n",
    "- Si $X,Y$ son variables independiente $~\\to H(X|Y) = H(X)~$, y conocer $Y$ no reduce la incertidumnbre en $X$. \n",
    "\n",
    "\n",
    "- Si $X=Y~ \\to ~H(X|Y) = 0$ y conocer $Y$ no deja ninguna incertidumbre residual en $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d4f38",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Información Mútua\n",
    "\n",
    "$H(X)$ es una medida de la cantidad de incertidumbre a priori que hay en $X$, es decir de la información que se  remueve al conocer un evento $x$. \n",
    "\n",
    "$H(X|Y)$ es una medida\n",
    "de la incertidumbre que queda en $X$ después de haber recibir  $y$. \n",
    "\n",
    "Necesariamente $H(X|Y)<H(X)$ puesto que no se puede remover más incertidumbre que la que hay originalmente en $X$. \n",
    "\n",
    "La diferencia es, por tanto, positiva $H(X)-H(X|Y)>0$, y cuantifica la cantidad de información *ganada* por el receptor. Se denomina  *información mutua*\n",
    "\n",
    "\n",
    "$$\n",
    " I(X,Y) = H(X)  + H(Y) - H(X,Y) \\geq 0 .\n",
    "$$\n",
    "\n",
    "\n",
    "La no-negatividad  de la información mutua será probada en breve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc418344",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Vemos que esta cantidad es simétrica en $X\\leftrightarrow Y$ y se puede escribir\n",
    "\n",
    "$$\n",
    "I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)    \\, .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb34c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Por tanto, $I(X,Y)$ es una medida de las *correlaciones* que hay entre ambas variables.\n",
    "\n",
    "- Si no hay ninguna correlación, entonces haber medido $Y$ no aporta nada, y la información mutua es cero\n",
    "$I(X,Y)=0$. \n",
    "\n",
    "\n",
    "- Si están perfectamente correlacionadas $I(X,Y)=H(X)$, entonces hay una correlación perfecta entre emision y recepción  y la información transmitida alcanza\n",
    "su máximo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa7037",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "La no-negatividad de la información mutua recibe otra denominación equivalente: **sub-aditividad**\n",
    "\n",
    "::::::{card} \n",
    "<b>Corolario</b>: </i>sub-aditividad</i> \n",
    "\n",
    "^^^\n",
    "\n",
    "La entropía de Shannon es sub-aditiva. Dadas dos variables aleatorias $X$ e $Y$\n",
    "\n",
    "    \n",
    "$$ H(X,Y)   \\leq H(X) + H(Y) $$\n",
    "    \n",
    "donde la igualdad se satura si no existe ninguna correlación entre ambas variables \n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb3289",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropía relativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2ad38",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "La entropía relativa  es una medida de la *distancia* entre las distribuciones de probabilidad $p(x)$ y $q(x)$, definidas sobre el mismo espacio muestral $x\\in X$\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(p|| q) &=& \\sum_x p(x) \\big( \\log p(x)- \\log q(x) \\big)  \\\\\n",
    "&=&  - H(X) -  \\sum_x p(x) \\log q(x)\\, .\n",
    "\\end{eqnarray}\n",
    "\n",
    "A veces se denomina a esta distancia: *divergencia* (de Kuhllback Leibler).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32a13e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>: </i>desigualdad de Gibbs</i> \n",
    "\n",
    "^^^\n",
    "\n",
    "La entropía relativa es <i>no-negativa</i>. Es decir, para dos distribuciones arbitrarias $p(x),q(x)$ se cumple que\n",
    "\n",
    "    \n",
    "$$\n",
    "H(p|| q) ~=~  \\sum_x p(x)  \\log p(x) -  \\sum_x p(x)  \\log q(x)  ~\\geq~ 0\n",
    "$$\n",
    "\n",
    "y la desigualdad se satura si y sólo si $p(x) = q(x)$ son distribuciones idénticas\n",
    "::::::\n",
    ":::{dropdown} Demostración\n",
    "\n",
    " En general tendremos\n",
    " \\begin{eqnarray}\n",
    " \\sum_x p(x)  \\log \\frac{p(x)}{q(x)} & ~= ~&   \\sum_x p(x) \\log\\frac{p(x)}{q(x)}    ~= ~ - \\sum_x p(x)   \\log\\frac{q(x)}{p(x)} \\nonumber\\\\\n",
    "&~\\geq ~& -  \\log  \\sum_x p(x)  \\frac{q(x)}{p(x)}   ~= ~ -  \\log  \\sum_x q(x)   \\nonumber\\\\\n",
    "&=&  -\\log 1\\nonumber =  0  \\nonumber\n",
    "\\end{eqnarray} \n",
    "\n",
    "donde la desigualdad se sigue del hecho de que $- \\log x$ es una función convexa, en cualquier base. \n",
    "Para la igualdad,  $p(x) = q(x)$ es condición suficiente. Para ver que es necesaria consultar referencias.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5c48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tomando varios casos particulares podemos ahora probar algunas afirmaciones realizadas anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc93f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La entropía de Shannon está acotada por la dimensión, $N$ del espacio muestral\n",
    "\n",
    "\n",
    "$$\n",
    " H(X) \\leq \\log N\n",
    " $$\n",
    " \n",
    ":::{dropdown} Demostración\n",
    "\n",
    "\n",
    "Basta tomar \n",
    "$q_X(x) = 1/N$. Entonces\n",
    " $ H(p_X|| q_X) =  - H(X)  +\\log N \\geq 0$.\n",
    "\n",
    " \n",
    "La desigualdad  se satura cuando $X$ también es una variable equiprobable $p_X(x)=q_X(x) = 1/N ~\\Rightarrow ~ H(p_X|| q_X) = 0$.\n",
    "\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6cd73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La entropía de Shannon es sub-aditiva\n",
    "\n",
    "\n",
    "$$\n",
    "H(X,Y) \\leq  H(X) + H(Y) \n",
    "$$\n",
    "\n",
    ":::{dropdown} Demostración\n",
    "\n",
    "  Tomemos ahora sobre la variable aleatoria $XY \\to \\{(x,y)\\}$, para $p_X$ la distribución combinada $p_{X,Y}$ y para $q_X$ la  factorizada $ p_Xp_Y$, donde $p_X(x) = \\sum_y p_{XY}(x,y)$ y $p_Y(y) = \\sum_x p_{XY}(x,y)$ son las distribuciones marginales \n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(p_{X,Y}(x, y) || p_X(x) p_Y(y)) \n",
    "&=& -  H(X,Y) - \\sum_{x,y} p_{XY}(x,y) \\log p_X(x)  - \\sum_{x,y} p_{XY}(x,y) \\log p_Y(y) \\nonumber\\\\\n",
    "&=& -  H(X,Y) - \\sum_{x} p_{X}(x) \\log p_X(x)  - \\sum_{y} p_{Y}(y) \\log p_Y(y) \\nonumber\\\\\n",
    "&=&  - H(X,Y) + H(X) + H(Y)  \\\\\n",
    "&=& ~ I(X,Y) \\geq 0  \\rule{0mm}{8mm}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Esta desigualdad prueba que la información mútua es no-negativa. Equivalentemente\n",
    "\n",
    "\n",
    "$$\n",
    "H(X,Y) \\leq  H(X) + H(Y) \n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75c76b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropías de información cuánticas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eff98b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Supongamos una fuente aleatoria clásica que genera letras de un alfabeto $X =\\{x_a, p_a\\}$ con probabilidad $p_a = p(x_a)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d5bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "La *incertidumbre  a priori* (información) vienen dada por la entropía de Shannon \n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0cda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para transmitir un mensaje usando un *canal cuántico* preparamos estados $x_i \\to \\ket{\\psi_i}$ con $i$ aparatos  y los enviamos sucesivamente. \n",
    "\n",
    "De esta manera hemos creado una *fuente de señal cuántica*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b6310",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Desde el punto de vista del receptor, se trata de una superposición estadística incoherente de estados $X = \\{\\ket{\\psi_a},p_a\\}$ que son recibidos con probabilidad $p_a = p(\\ket{\\psi_a})$.\n",
    "\n",
    "Para descifrar el mensaje, el receptor debe adivinar qué estados componen el estado que le llega realizando medidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b483de9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El <b>operador densidad</b> es el objeto matemático que caracteriza la mezcla estadística que se recibe\n",
    "\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{a} p_a \\ket{\\psi_a}\\bra{\\psi_a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f67f85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Notemos que \n",
    " \n",
    " - no hemos demandado que $\\ket{\\psi_a}$ sea un conjunto de vectores ortogonales. En general no lo será.\n",
    "\n",
    "\n",
    " - el número de vectores y de letras $a=1,2, ...$, puede ser mayor o menor que la dimensión del espacio de Hilbert del sistema cuántico \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4e360",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por ser hermítico, siempre podemos escribir $\\rho$ en su *representación espectral*\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{i=1}^N \\lambda_i \\ket{\\lambda_i}\\bra{\\lambda_i}\n",
    "$$\n",
    "\n",
    "donde $\\lambda_i$ son los autovalores y $\\ket{\\lambda_i}$ sus autovectores, que forman una base ortonormal $\\braket{\\lambda_i}{\\lambda_j} = \\delta_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20903b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esta representación, hace referencia a un *aparato hipotético* asociado a medidas proyectivas $\\{M_i = \\ketbra{\\lambda_i}{\\lambda_i}\\}$ \n",
    "\n",
    "Denominamos a la variable aleatoria cuántica  $\\hat C = \\{\\ket{\\lambda_i},\\lambda_i\\}$ *colectivo canónico*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977ae2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ent_vonNeumann'></a>\n",
    "\n",
    "### Entropía de Von Neumann\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de20cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "El procedimiento descrito nos confronta con dos colectivos \n",
    "\n",
    "- el  *original*, asociado a la preparación $X = \\{x_a, p_a\\} \\to \\{\\ket{\\psi_a},p_a\\}$\n",
    "\n",
    "\n",
    "- el *canónico*, asociado a la diagonalización de $\\rho \\to   C = \\{\\ket{\\lambda_i},\\lambda_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70240b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cada colectivo lleva asociada una entropía de Shannon\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& -\\sum_a p_a \\log p_a \\\\\n",
    "H(C) &=& -\\sum_{i=1}^N \\lambda_i \\log \\lambda_i\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e4215",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La clave de la segunda expresión  es que, por definición, es equivalente a la siguiente  \n",
    "\n",
    "\n",
    "$$\n",
    "H(C) = -\\Tr (\\rho \\log \\rho)\n",
    "$$\n",
    "\n",
    "\n",
    "La ventaja de escribirlo de esta manera es que es *independiente* de la base. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d9554",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Definición</b>: </i>entropía de von Neumann</i> \n",
    "\n",
    "^^^\n",
    "    \n",
    "$$\n",
    "S(\\rho) = -\\Tr (\\rho\\log \\rho)\n",
    "$$  \n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb4326",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observar que, escrito de esta manera,  $S(\\rho)$\n",
    "\n",
    "- no hace referencia a *ninguna base* de estados concreta. \n",
    "\n",
    "\n",
    "- está unívocamente determinada para cada estado $\\rho$ \n",
    "\n",
    "\n",
    "- tampoco hace referencia al procedimiento de preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266f4d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En definitiva: *podemos asignar una  entropía de von Neumann a todo operador densidad* $\\rho$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5924a4d",
   "metadata": {},
   "source": [
    "::::::{admonition} Ejercicio\n",
    ":class: tip\n",
    "\n",
    "Escribe una función $S\\_entropy(\\rho)$ que devuelva la entropía de Von Neumann asociada a un estado $\\rho$\n",
    "escrito como una matriz en la base canónica.\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac593849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Propiedades de la entropía de Von Neumann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375bb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **acotación**:\n",
    "sea $N=$ la <u>dimensión de $\\Hil$</u>, la entropía de von Neumann está acotada por\n",
    "\n",
    "$$0 \\leq S(\\rho)\\leq \\log N$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f1785",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "-  en un <i>estado puro</i>, la entropía de von Neumann  es nula \n",
    "\n",
    "\n",
    "$$\n",
    "S(\\rho ) = 0 ~~~\\Longleftrightarrow ~~~\\rho^2 = \\rho \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275dfa1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  en un estado <i>maximalmente mezclado</i>, la entropía de un estado es maximal \n",
    "\n",
    "\n",
    "$$\n",
    "S(\\rho) = \\log N ~~~\\Longleftrightarrow ~~~\\rho = \\frac{1}{N} I\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348cacb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **concavidad**: \n",
    "\n",
    "\n",
    "$ S(\\rho)$ es una función cóncava de su argumento $\\rho$. Para cualquier linea recta que interpole entre $\\rho_1$ y $\\rho_2$\n",
    "\n",
    "\n",
    "$$ S\\left(\\rule{0mm}{4mm}\\lambda \\rho_1 +(1-\\lambda) \\rho_2 \\right) \\geq \\lambda S(\\rho_1) +(1-\\lambda) S(\\rho_2) $$\n",
    "\n",
    "donde $\\lambda \\in (0,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02575121",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{admonition} Notar\n",
    ":class: note\n",
    "\n",
    "La concavidad de $S$ se generaliza a combinaciones lineales. Sea $\\rho = \\sum_{i=1}^r p_i \\rho_i$, donde $~\\sum_{i=1}^r p_i = 1$  \n",
    "$$\n",
    "S(\\rho ) \\geq \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9709d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The proof will be given later by means of the subaditivity property. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f299ed4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **invariancia**: \n",
    " la entropía de von Neumann es invariante bajo *transformaciones unitarias*\n",
    "\n",
    "$$ \n",
    "S(\\rho) = S(U^\\dagger \\rho U)\n",
    "$$\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb02b95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En particular esto implica que la entropía de von Neumann de un sistema aislado es constante en el tiempo \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "S(\\rho(t)) = S(U(t)\\rho(0) U(t)^\\dagger) = S(\\rho(0))\n",
    "$$\n",
    "\n",
    "Es decir\n",
    "\n",
    "$$\n",
    "\\frac{dS(t)}{dt} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cc29e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Inversamente, puede probarse que si $dS(t)/dt \\neq 0$ entonces\n",
    "\n",
    "- el sistema es abierto\n",
    "\n",
    "\n",
    "- $ \\displaystyle \\frac{dS(t)}{dt} >  0 ~~ $ la entropía sólo puede crecer\n",
    "\n",
    "En este caso hablamos de *evolución incoherente* o *decoherencia*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de64a64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id= 'entrop_rel'></a>\n",
    "### Entropía relativa\n",
    "\n",
    "\n",
    "Definimos la entropía relativa por analogía formal con el caso clásico. Sean $\\rho$ y $\\sigma$ dos estados cuánticos, la entropía relativa es una medida de distancia que se anula cuando son iguales\n",
    "\n",
    "\n",
    "$$\n",
    "S(\\rho \\| \\sigma) = \\Tr \\rho(\\log\\rho - \\log \\sigma)\n",
    "$$\n",
    "\n",
    "\n",
    "La desigualdad de Gibbs para $H(X\\| Y)$ tiene un resultado paralelo  para $S(\\rho\\|\\sigma)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa7e1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>: </i>desigualdad de Klein</i> \n",
    "\n",
    "^^^\n",
    "\n",
    "La entropía relativa es no-negativa\n",
    "$$\n",
    "S(\\rho \\| \\sigma) \\geq 0\n",
    "$$  \n",
    "\n",
    "y se anula si y sólo si $\\rho = \\sigma$\n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb01cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ent_for_med'></a>\n",
    "### Entropías de preparación y de medida   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ffde5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Entropía de preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a459c",
   "metadata": {},
   "source": [
    "Existen infinitos colectivos $X = \\{\\ket{\\psi_a},p_a\\}~,~  \\tilde X = \\{\\ket{\\tilde \\psi_i},\\tilde p_i\\},...$ que son descritos por el mismo operador densidad\n",
    "\n",
    "$$\n",
    "\\rho ~=~ \\sum_{a=1}^r p_a \\ket{\\psi_a}\\bra{\\psi_a} ~= ~ \\sum_{i=1}^s \\tilde p_i \\ket{\\tilde\\psi_i}\\bra{\\tilde\\psi_i}  ~=~~ ...\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769b57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- Cada colectivo lleva asociado una entropía de Shannon $H(X), H(\\tilde X) ,...$ diferente. \n",
    "\n",
    "\n",
    "- Sin embargo, la entropía de von-Neumann $S(\\rho)$ es la misma para todos porque sólo depende de $\\rho$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3ffbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Definición</b>: </i>entropía de preparación</i> \n",
    "\n",
    "^^^\n",
    "    \n",
    "Para cada colectivo $X = \\{\\ket{\\psi_a},p_a\\}$ que prepara un estado  $\\rho = \\sum_a p_a\\ket{\\psi_a}\\bra{\\psi_a}$ definimos la <i>entropía de preparación</i> como la diferencia  \n",
    " \n",
    "    \n",
    "$$\\Delta(X,\\rho) = H(X) - S(\\rho )$$\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb583d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>:  \n",
    "\n",
    "^^^\n",
    "La entropía de preparación es no-negativa $\\Delta(X,\\rho) \\geq 0$, es decir\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "S(\\rho ) ~~\\leq ~~ H(X)\n",
    " \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "La desigualdad se satura para una preparación $X$ en la que los estados $\\{\\ket{\\psi_a}\\}$ sean ortogonales\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f95d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La demostración es larga y no la haremos. \n",
    "El resultado es plausible porque $H(X) \\leq \\log(r)$ donde $r$ el número de *letras* en el colectivo $\\ket{\\psi_a}, \\, a = 1,...,r$ que no está acotado, mientras que $S(\\rho)\\leq \\log N$ está acotado por la dimensión del espacio de Hilbert $\\Hil$. \n",
    "\n",
    "\n",
    "\n",
    "- Por otro lado, si pedimos que los estados sean ortogonales, entonces $r\\leq N$. Podemos completar hasta formar una base $\\{\\ket{\\psi_a}\\} \\, a = 1,..,N$. Como $S(\\rho)$ es invariante bajo transformaciones unitarias, es igual a la escrita en la base de autoestador $\\{\\ket{\\lambda_a}\\}$ es decir $H$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c40550",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Si los estados de la fuente $X = \\{\\ket{\\psi_a},p_a\\}$ no son ortogonales $S< H$. Pero los \n",
    "$\\{\\ket{\\psi_a}\\}$  no se pueden distinguir $\\Rightarrow$ no hay un observable que permita recuperar toda la información codificada en el mensaje clásico.\n",
    "\n",
    "\n",
    "$\\Rightarrow \\rho$ transmite menos información por el canal cuántico que la contenida en el mensaje clásico original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9f650",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{admonition} Ejemplo 1 (Estados ortogonales) \n",
    ":class: tip\n",
    "\n",
    "\n",
    "\n",
    "Supongamos que Alice tiene una fuente aleatoria  de estados ortogonales\n",
    "\n",
    "    \n",
    "$$X = \\{ \\ket{\\psi_i}, p_i\\} = \\{ (\\ket{0}, p_0= 1/4), (\\ket{1},p_1 = 3/4)\\}$$\n",
    "\n",
    "Bob describe el sistema mediante la matriz densidad\n",
    "\n",
    "$$\n",
    "\\rho = p_0\\ket{0}\\bra{0} + p_1\\ket{1}\\bra{1} = \\begin{bmatrix} p_0 & 0 \\\\ 0 & p_1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "y la entropía de Shannon asociada será\n",
    "\n",
    "\\begin{eqnarray}\n",
    "S(\\rho) &=&  -\\Tr \\rho\\log \\rho = - \\Tr \\left( \\begin{bmatrix} p_0 & 0 \\\\ 0 & p_1 \\end{bmatrix}  \\begin{bmatrix} \\log p_0 & 0 \\\\ 0 & \\log p_1 \\end{bmatrix} \\right) \\nonumber\\\\\n",
    "&=& \\rule{0mm}{5mm}\n",
    "-p_0\\log p_0 - p_1 \\log p_1 = H(p_0,p_1) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "Entonces, para estados ortogonales, las entropías de  Von Neuman y de Shannon son iguales\n",
    "    \n",
    "    \n",
    "\\begin{eqnarray}\n",
    "S(\\rho)\\rule{0mm}{8mm}&=& -\\frac{1}{4}\\log \\frac{1}{4} - \\frac{3}{4} \\log \\frac{3}{4}    = 0.81 \\, \\hbox{bits} = \n",
    "  H(X)   \\, . \\nonumber\n",
    "\\end{eqnarray}\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f5e41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{admonition} Ejemplo 2 (Estados no-ortogonales) \n",
    ":class: tip\n",
    "\n",
    "\n",
    "    \n",
    "Sea ahora otra fuente de Alice que produce el colectivo de estados con idénticas probabilidades $p_i$\n",
    "\n",
    "    \n",
    "$$\\{ \\ket{\\psi_i}, p_i\\} = \\left\\{\\rule{0mm}{4mm} (\\ket{0}, p_0= 1/4)\\, , \\, (\\ket{+},p_+ = 3/4)\\right\\}$$\n",
    "    \n",
    "Bob ahora escribe la matriz densidad\n",
    "    \n",
    " \n",
    "$$\n",
    "\\rho = \\frac{1}{4}\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} + \\frac{3}{8} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 5 & 3 \\\\ 3 & 3 \\end{bmatrix} \\,\n",
    "$$        \n",
    " \n",
    "Diagonalizando obtenemos autovalores\n",
    "$$\\lambda_i = \\frac{1}{2} \\pm \\frac{1}{4} \\sqrt{\\frac{5}{2}}$$ \n",
    "     \n",
    "Ahora calculamos la entropía de Shannon \n",
    "    \n",
    "$$\n",
    "S(\\rho) = -\\sum_i \\lambda_i \\log \\lambda_i \\, =\\,  0.485\\, \\hbox{bits} \\, <\\,   0.81   \\, \\hbox{bits} ~=~   H(X)\n",
    "$$\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2857a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{admonition} Notar\n",
    ":class: note\n",
    "\n",
    "\n",
    "Que $S$ sea menor que $H$ también parece indicar que un alfabeto cuántico $X= \\{\\ket{\\psi_a},p_a\\}$ podría admitir una codificación con  menos recursos que uno clásico. Es decir,  una compresión mayor. \n",
    "\n",
    "\n",
    "La demostración de este hecho es el teorema de Schuhmacher.\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7d833",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropía de medida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e461e",
   "metadata": {},
   "source": [
    "Bob recibe un sistema en un estado $\\rho$ y le aplica una medida proyectiva $\\{E_m = P_m\\}$, donde \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "P_l^2 = P_l~,~~ P_m P_n = P_m\\delta_{mn}~,~~ \\sum_m P_m = I\n",
    "$$\n",
    "\n",
    "Si <u>no registra el resultado</u>, la *medida no selectiva* tendrá el siguiente efecto sobre el estado\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\rho ~~ \\stackrel{\\{P_m\\}}{\\longrightarrow} ~~ \\rho' = \\sum_m P_m \\rho P_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce26a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>:  \n",
    "\n",
    "^^^\n",
    "    \n",
    "En una medida proyectiva no-selectiva, la entropía no disminuye, \n",
    "$\n",
    "S(\\rho') \\geq S(\\rho)\n",
    "$.    \n",
    "La desigualdad se satura cuando la base  de la medida proyectiva diagonaliza $\\rho = \\sum_m\\lambda_m P_m $.\n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae065f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    ":::{dropdown} Demostración\n",
    "    \n",
    "Queremos demostrar que \n",
    "$$\n",
    "0 ~\\leq -S(\\rho) + S(\\rho') = -S(\\rho) -\\tr (\\rho'\\log \\rho')\n",
    "$$\n",
    "    \n",
    "Conocemos una desigualdad muy parecida, la desigualdad  de Klein, \n",
    "    \n",
    "$$\n",
    "0~\\leq ~S(\\rho\\|\\rho') ~=~ -S(\\rho) -\\tr (\\rho\\log \\rho')\n",
    "$$\n",
    "    \n",
    "Sería suficiente con demostrar  que los segundos términos son iguales $\\tr (\\rho\\log \\rho') = \\tr (\\rho'\\log \\rho')$\n",
    "\n",
    "$$\n",
    "\\tr (\\rho'\\log \\rho') = \\tr\\left[ \\sum_l  P_l \\rho P_l \\log \\rho' \\right] \n",
    "$$\n",
    "Examinemos: \n",
    "\\begin{eqnarray}\n",
    "P_l \\rho' &=& P_l\\sum_m P_m \\rho P_m = \\sum_m P_l\\delta_{lm}\\rho P_m = P_l\\rho P_l \\nonumber \\\\\n",
    "\\rho'P_l  &=& \\sum_m P_m \\rho P_mP_l = \\sum_m P_m\\rho P_l \\delta_{lm} = P_l\\rho P_l\\nonumber  \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "De aquí se deduce que $~\\rho' P_l = P_l \\rho' ~\\Rightarrow ~\\log\\rho' P_l = P_l \\log \\rho'~$,\n",
    "y por tanto\n",
    "    \n",
    "pero \n",
    "$$\n",
    "\\tr(\\rho'\\log \\rho') =  \\tr\\left[\\sum_l\\left( P_l \\rho \\log \\rho' P_l\\right)\\right]= \\tr \\left[\\left(\\sum_l P_l^2\\right) \\rho \\log \\rho'\\right] = \\tr(\\rho\\log \\rho')\n",
    "$$\n",
    "y así  llegamos al resultado deseado.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c6957",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "::::::{admonition} Ejercicio\n",
    ":class: tip\n",
    "\n",
    " Trabaja en $\\Hil$ de dimensión 6. De forma aleatoria, define un colectivo $\\{\\ket{\\psi_a},q_a\\},\\, a = 0,...r-1$. \n",
    "Haz una medida no-selectiva proyectiva en la base $\\ket{i}$  computacional. Obtén la variación de entropía por la \n",
    "medida. \n",
    "\n",
    "Repíte, haciendo la medida en la base $\\ket{\\lambda_i}$ de autoestados de $\\rho$.   \n",
    "    \n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5a6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropía de mezclas estadísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73a128",
   "metadata": {},
   "source": [
    "La idea es comparar las entropías de una serie de estados $\\rho_i\\, i=1,2,...r$ con la de una mezcla estadística de estados mezclados\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{i=1}^r p_i \\rho_i\n",
    "$$\n",
    "con $p_i\\geq 0, \\, \\sum_{i=1}^r p_i =1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241f1a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>:  \n",
    "\n",
    "^^^\n",
    "    \n",
    "Sea $\\rho = \\sum_{i=1}^r p_i\\rho_i$ una mezcla estadística de estados  $\\rho_i$ con probabilidades $p_i\\geq 0,\\, \\sum_{i=1}^r p_i =1$. Se demuestran las siguientes desigualdades\n",
    " \n",
    "    \n",
    "$$\\fbox{$\n",
    "~\\sum_{i=1}^r p_i S(\\rho_i) ~~\\leq~  S(\\rho) ~\\leq~ \\sum_{i=1}^r p_i S(\\rho_i) + H(\\{p_i\\}) ~\n",
    "$}\n",
    "$$   \n",
    "\n",
    "\n",
    "La desigualdad de la derecha se satura cuando $\\rho_i$ tienen soporte en subespacios mutuamente ortogonales.    \n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9097952",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ":::{dropdown} Demostración\n",
    "\n",
    "La desigualdad de la izquierda ya la hemos mencionado, y es la propiedad de <b>concavidad</b> de la entropía.\n",
    "Su demostración es sencilla usando la desigualdad triangular que será demostrada más adelante.\n",
    "\n",
    "Para demostrar la desigualdad de la derecha empezaremos probándola para el caso en que $\\rho_a = \\ketbra{\\psi_i}{\\psi_i}$ son estados puros, **no necesariamente ortogonales**.\n",
    "\n",
    "Introduzcamos un sistema auxiliar $B$ con dimensión $d_B\\geq r$ y  base ortonormal $\\{\\ket{i}\\}$ y definamos $\\rho_{AB} = \\ketbra{AB}{AB}$ en términos del estado entrelazado\n",
    "\n",
    "$$\n",
    "\\ket{AB} = \\sum_{i=1}^r \\sqrt{p_i} \\ket{\\psi_i}\\ket{i}\n",
    "$$\n",
    "\n",
    "Al ser $\\rho_{AB}$ puro sus trazas parciales coinciden y, por tanto, sus entropías también\n",
    "\n",
    "$$\n",
    "S(B) = S(A) = S\\big(\\sum_{i} p_i\\ketbra{\\psi_i}{\\psi_i}\\big) = S(\\rho)\n",
    "$$\n",
    "\n",
    "A continuación efectuamos una medida proyectiva y no-selectiva en $B$ usando los proyectores $P_i^B = \\ketbra{i}{i}$\n",
    "\n",
    "$$\n",
    "\\rho_B'= \\sum_i P_i^B\\rho_B P_i^B = \\sum_i p_i \\ketbra{i}{i}\n",
    "$$\n",
    "\n",
    "Hemos probado que, una medida no-selectiva sólo puede aumentar la entropía, es decir\n",
    "\n",
    "$$\n",
    "S(B)= S(\\rho) \\leq S(\\rho_B') = - \\sum_i p_i\\log p_i = H(\\{p_i\\})\n",
    "$$\n",
    "\n",
    "Por tanto, cuando $\\rho_i$ son estados puros, tenemos que\n",
    "\n",
    "$$\n",
    "S (\\rho) \\leq H(\\{p_i\\}) + \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    "\n",
    "donde hemos sumado el último término que es cero. La desigualdad se satura si los $\\ket{\\psi_i}$ son ortogonales.\n",
    "\n",
    "\n",
    "\n",
    "Ahora podemos abordar el caso general en el que $\\rho_i$ son mezclados. La descomposición espectral de cada $\\rho_i$ es\n",
    "\n",
    "$$\n",
    "\\rho_i = \\sum_{j=1}^N \\pi^i_j \\ketbra{e^i_j}{e^i_j}\n",
    "$$\n",
    "\n",
    "donde las $r$ bases $\\{\\ket{e^i_j}\\, , j=1,...,N\\}$ son en principio diferentes. Ahora podemos escribir\n",
    "\n",
    "$$\n",
    "\\rho ~=~ \\sum_{i=1}^r \\sum_{j=1}^N p_i \\pi^i_j \\ketbra{e^i_j}{e^i_j}  ~= ~\\sum_{i,j} q_{ij}\\rho_{ij}\n",
    "$$\n",
    "\n",
    "donde hemos considerador $\\rho$ como una mezcla de estados puros $\\rho_{ij}$. A este caso le podemos aplicar el resultado encontrado para estados puros\n",
    "\n",
    "\n",
    "$$\n",
    "S(\\rho) \\leq H(\\{q_{ij}\\} = -\\sum_{ij} p_i\\pi^i_j \\log(p_i\\pi^i_j) = -\\sum_{ij} p_i\\pi^i_j(\\log p_i + \\log \\pi^i_j)\n",
    "= -\\sum_i p_i \\log p_i -\\sum_i p_i\\big( \\sum_j \\pi^i_j \\log \\pi^i_j \\big)\n",
    "$$\n",
    "\n",
    "donde hemos usador que $\\sum_j \\pi^i_j = \\tr(\\rho_i) = 1$. Ahora reconocemos en los dos últimos términos las funciones $H(\\{p_i\\})$ y  $S(\\rho_i)$, es decir\n",
    "\n",
    "$$\n",
    "S(\\rho) \\leq H(\\{p_i\\}) + \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    " \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c19b4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La desigualdad de la izquierda es la propiedad de concavidad cuyo contenido es que la entropía de una mezcla es mayor que la de sus partes. \n",
    "\n",
    "La diferencia es una cantidad importante que *conviene maximizar* ya que aumenta la cantidad de información del sistema. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d029a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Definición</b>: </i>información de Holevo</i> \n",
    "\n",
    "^^^\n",
    "    \n",
    "La <i> información de Holevo </i> de un estado $\\rho = \\sum_i p_i \\rho_i$ se define por el incremento de entropía asociado a la mezcla estadística \n",
    "\n",
    "    \n",
    "$$\n",
    "\\chi = S(\\rho) - \\sum_i p_i S(\\rho_i)    \n",
    "$$\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848322c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Del teorema anterior, restando la cantidad $\\rho = \\sum_{i=1}^r p_i \\rho_i$ en todos los miembros, se verifica la desigualdad siguiente para la información de Holevo\n",
    "\n",
    "::::::{card} \n",
    "<b>Corolario</b>:  \n",
    "\n",
    "^^^\n",
    "En una mezcla estadística $\\rho = \\sum_{i=1}^r p_i \\rho_i$ la información de Holevo se encuentra acotada en la forma \n",
    "\n",
    "    \n",
    "$$ \n",
    "0 \\leq \\chi(\\rho) \\leq H(\\{p_i\\}) \n",
    "$$\n",
    "    \n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160fd67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{admonition} Ejercicio\n",
    ":class: tip\n",
    "\n",
    " Trabaja en $\\Hil$ de dimensión 6. De forma aleatoria, define tres colectivos $\\{\\ket{\\psi_a},q_a\\}_I$ con $a = 0,...r_a-1$ y  $I=0,1,2$. Con 3 probabilidades aleatorias, $\\{p_i\\}\\, i=0,1,2$ considera la mezcla estadística $\\rho = \\sum_i p_i \\rho_i$.  Calcula la información de Holevo y verifica las cotas. \n",
    "    \n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298058be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"ent_comp\"></a>\n",
    "## Entropías cuánticas de sistemas compuestos</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952f25a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Depués de la información contenida en un estado, la cantidad importante que deseamos conocer es el *grado de correlación* que guardan dos sistemas $A$ y $B$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1677062",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Considerados de forma conjunta, el sistema bipartito aislado  $AB \\sim \\Hil_A\\otimes \\Hil_B$ \n",
    "\n",
    "recordemos que toda la  información accesible   para observadores   que puedan medir en $AB$ ($A$, $B$) está contenida en $\\rho$, $(\\rho_A,\\rho_B)$\n",
    "\n",
    "en particular, la entropía de von Neumann $S(\\rho)$ mide la incertidumbre de Shannon asociada a una preparación mediante un conjunto de medidas proyectivas \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c04e93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropía de entrelazamiento\n",
    "<a id='ent_entrelaz'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d39ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "es natural pensar que el grado de entrelazamiento entre $A$ y $B$ tenga un reflejo en los estados parciales $\\rho_A$ y $\\rho_B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6b0c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Definición</b>: </i>Entropía de entrelazamiento</i> \n",
    "\n",
    "^^^\n",
    "\n",
    "La <b>entropía de entrelazamiento</b> es\n",
    "la entropía de Von Neumann de un sub-sistema obtenido a partir de la traza parcial sobre su complemento\n",
    "\n",
    "\n",
    "$$\n",
    "S(\\rho_A) = \\Tr \\rho_A \\log \\rho_A~~~~~~\\hbox{con} ~~~~~\\rho_A = \\Tr_B \\rho\n",
    "$$\n",
    "   \n",
    "$$\n",
    "S(\\rho_B) = \\Tr \\rho_B \\log \\rho_B~~~~~~\\hbox{con} ~~~~~\\rho_B = \\Tr_A \\rho\n",
    "$$  \n",
    "    \n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4b5af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Notación**: salvo advertencia, cuando tratemos con sistemas compuestos, denotaremos \n",
    "\n",
    "$$ \n",
    "S(AB) = S(\\rho) ~~~~~~~~~ S(A) = S(\\rho_A)  ~~~~~~~~~~ S(B) = S(\\rho_B)\n",
    "$$\n",
    "\n",
    "donde se entiende que se trata de los sistemas obtenidos mediante las trazas parciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acacbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Entropía de un estado no correlacionado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f5d48",
   "metadata": {},
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>: </i></i> \n",
    "\n",
    "^^^\n",
    "\n",
    "Para un estado no-correlacionado encontramos\n",
    "$$\n",
    "S(\\rho) = S (\\rho_A\\otimes \\rho_B) = S(\\rho_A) + S(\\rho_B)\n",
    "$$\n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68153f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{dropdown} Demostración\n",
    "Trabajando con las resoluciones espectrales de $\\rho_A$ y $\\rho_B$ encontramos que\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log (\\rho_A\\otimes \\rho_B) &=& \\big( \\sum_{i,a} \\log(\\lambda_i \\mu_a) \\ketbra{\\lambda_i \\mu_a}{\\lambda_i\\mu_a}\\big) \\\\\n",
    "&=& \\big( \\sum_{i,a} (\\log\\lambda_i+ \\log \\mu_a)  \\ketbra{\\lambda_i \\mu_a}{\\lambda_i\\mu_a}\\big)  + \\\\\n",
    "&=& \\sum_{i} \\log \\lambda_i \\ketbra{\\lambda_i}{\\lambda_i}\\otimes \\sum_a \\ketbra{\\mu_a}{\\mu_a} + \n",
    "\\sum_{i}  \\ketbra{\\lambda_i}{\\lambda_i}\\otimes \\sum_a \\log \\mu_a \\ketbra{\\mu_a}{\\mu_a} \\\\\n",
    "&=& \\log\\rho_A\\otimes I + I\\otimes \\log \\rho_B\n",
    "\\end{eqnarray*}  \n",
    "Entonces\n",
    "\\begin{eqnarray*}\n",
    "S(\\rho_A\\otimes \\rho_B) &=& -\\tr_{AB} \\left[(\\rho_A\\otimes \\rho_B)\\log (\\rho_A\\otimes\\rho_B)\\right] \\\\\n",
    " &=& -\\tr_{AB} \\left[(\\rho_A\\otimes \\rho_B)(\\log\\rho_A\\otimes I + I\\otimes \\log \\rho_B)\\right] \\\\\n",
    "&=&- \\tr_{AB}\\left[\\rho_A\\log\\rho_A\\otimes \\rho_B + \\rho_A \\otimes \\rho_B \\log\\rho_B\\right]\\\\\n",
    "&=& \\tr(\\rho_A\\log\\rho_A)\\otimes \\tr \\rho_B + \\tr\\rho_A \\otimes \\tr(\\rho_B \\log\\rho_B)\\\\\n",
    "&=& S(A) + S(B)\n",
    "\\end{eqnarray*}\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377b88e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Esta noción de no-correlación es análoga a la que existe en probabilidad clásica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc33950",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Clásicamente hay varias entropías que juegan un papel central, la entropía condicional $H(X|Y)$, la entropía relativa $H(X\\|Y)$ y la información mutua $I(X,Y)$.\n",
    "Las tres admiten interpretaciones en términos de incertidumbres y expectativas. \n",
    "\n",
    "\n",
    "Podemos definir,  cantidades *formalmente análogas* en el contexto cuántico, a pesar de que la interpretación probabilística no es tan evidente, o es desconocida. \n",
    "\n",
    "Ya hemos definido antes la [entropía relativa](#entrop_rel), y su importante propiedad de positividad. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8531c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id ='infor_mutua'></a>\n",
    "### Información mutua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82f71f",
   "metadata": {},
   "source": [
    " La definición de información mutua es la misma, intercambiando la entropía de Shannon por la de von Neumann\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "I(A,B) = S(A) + S(B) - S(AB) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a3499",
   "metadata": {
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>: </i>positividad de la Información Mutua</i> \n",
    "\n",
    "^^^\n",
    "\n",
    "$$\n",
    "I(A,B) \\geq 0\n",
    "$$\n",
    "    \n",
    "y la desigualdad se satura $I(A,B) = 0$ si y solo si $\\rho = \\rho_A\\otimes \\rho_B$ es factorizable\n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d5aef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{dropdown} Demostración\n",
    "\n",
    "    \n",
    "Consideremos la entropía relativa asociada a los estados $\\rho  = \\rho_{AB}$ y  $\\sigma = \\rho_A\\otimes \\rho_B$\n",
    "\n",
    "Entonces, por la desigualdad de Klein\n",
    "    \n",
    "\n",
    "\\begin{eqnarray}\n",
    "0\\leq S(\\rho\\|\\sigma) &=& \\tr\\left(\\rho_{AB}(\\log \\rho_{AB} - \\log (\\rho_A\\otimes \\rho_B) \\rule{0mm}{4mm}\\right)\\nonumber\\\\  \\rule{0mm}{8mm}\n",
    "&=& \\tr \\left(\\rho_{AB}(\\log \\rho_{AB} - \\log (\\rho_A\\otimes I) -\\log ( I \\otimes \\rho_B) \\rule{0mm}{4mm} \\right)\n",
    "\\\\ \\rule{0mm}{8mm}\n",
    "&=& S(AB) - \\tr_{A}(\\tr_B\\rho_{AB}) - \\tr_B(\\tr_A\\rho_{AB}) \\\\ \\rule{0mm}{8mm}\n",
    "&=& S(AB) - S(A) - S(B) \n",
    "\\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "    \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b90cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropía condicional \n",
    "\n",
    "Por último, copiaremos la definición de la entropía condicional, a pesar de que  no existe una noción de probabilidad condicional cuántica\n",
    "\n",
    "$$\n",
    "S(A|B) = S(AB) - S(B)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082b3b",
   "metadata": {},
   "source": [
    "\n",
    "Aquí encontramos una característica genuina:\n",
    "a diferencia del caso clásico, $S(A|B)$ *puede ser negativa*. \n",
    "\n",
    "- Por ejemplo, si $AB$ es un estado puro $\\Rightarrow S(AB) = 0$\n",
    "\n",
    "\n",
    "\n",
    "- Sin embargo no puede ser *demasiado* negativa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc818b3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>:  \n",
    "\n",
    "^^^\n",
    "\n",
    "$$\n",
    "S(A|B) \\geq  - \\hbox{min}(S_A,S_B)\n",
    "$$  \n",
    "\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36432c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{dropdown} Demostración\n",
    "\n",
    "Por un lado, del hecho de que $S(AB)\\geq 0$ se sigue que\n",
    "$$\n",
    "S(A|B) \\geq - S(B)\n",
    "$$ \n",
    "    \n",
    "Por otro, vamos a acoplar $AB$ a un tercer sistema $C$ y considerar un estado puro $ABC$ (se entiende ($\\rho_{ABC} = \\ket{\\psi_{ABC}}\\bra{\\psi_{ABC}}$).\n",
    "    \n",
    "Entonces sabemos que $S(AB) = S(C)$ y que, también $S(B) = S(AC)$. Encontramos que\n",
    "    \n",
    "$$\n",
    "S(A|B) = S(AB)- S(B) = S(C) - S(AC) \\geq -S(A)\n",
    "$$\n",
    "    \n",
    "donde hemos utilizado la positividad de la información mutua entre $A$ y $C$.  \n",
    "\n",
    "Por tanto se sigue el enunciado\n",
    "\n",
    "$$\n",
    "S(A|B) \\geq   \\hbox{max}(-S_A,-S_B) =  - \\hbox{min}(S_A,S_B)\n",
    "$$  \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d7f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "La entropía condicional juega un papel fundamental en la posibilidad de establecer *protocolos de teleportación*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c4534",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='desig_triang'></a>\n",
    "#### Desigualdad triangular de Araki-Lieb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617ac16",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ahora podemos enunciar ciertas relaciones que guardan la entropía de un sistema $AB$ con la de sus partes $A$ y $B$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af613ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>:  \n",
    "\n",
    "^^^\n",
    "   \n",
    "La entropías de von Neumann de un sistema compuesto $AB$ y de sus partes $A,B$ satisfacen, en cualquier estado $\\rho$, la desigualdad siguiente\n",
    "\n",
    "\n",
    "$$\n",
    "|S(A) - S(B)| ~~ \\leq ~~ S(AB) ~~\\leq ~~S(A) + S(B)\n",
    "$$   \n",
    "\n",
    "::::::\n",
    "\n",
    "- La desigualdad de la izquierda recibe el nombre de  *desigualdad de Araki-Lieb*\n",
    "\n",
    "\n",
    "- La desigualdad de la derecha se conoce por *sub-aditividad* de la entropía. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45a2da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{dropdown} Demostración\n",
    "\n",
    "La desigualdad de la derecha se denomina <i>subaditividad</i>    y equivale a la positividad de la entropía relativa.\n",
    "\n",
    "\n",
    "Para demostrar la desigualdad de la izquierda (Araki-Lieb) recordamos la cota inferior para la entropía condicional\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    " S(A|B) = S(AB) - S(B) &\\geq & - \\hbox{min}(S_A,S_B) \\geq -S_A \\\\\n",
    " S(B|A) = S(AB) - S(A) &\\geq& - \\hbox{min}(S_A,S_B) \\geq -S_B \\rule{0mm}{6mm}\n",
    "\\end{eqnarray}  \n",
    "\n",
    "De aquí se sigue que\n",
    "$$\n",
    "S(AB) \\geq  |S(A)- S(B)|\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce7056",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La propiedad de subatividad permite encontrar una demostración sencilla de la  *concavidad* de la entropía de Von Neumann.\n",
    "\n",
    "::::::{card} \n",
    "<b>Corolario</b>:  \n",
    "\n",
    "^^^\n",
    "La entropía de von Neumann es una función <b>cóncava</b> de su argumento\n",
    "\n",
    "    \n",
    "$$\n",
    "\\sum_i p_i \\rho_i \\leq S(p_i \\rho_i) \n",
    "$$\n",
    "::::::\n",
    ":::{dropdown} Demostración\n",
    "Sean $\\{\\lambda_{i,a}\\}, a=1,...,d_A$ autovalores  $\\rho_i$, then\n",
    "$$\n",
    "S(\\rho_i ) = -\\sum_{a}\\lambda_{ia}\\log\\lambda_{ia}\n",
    "$$\n",
    "\n",
    "Let us consider an auxiliary system $B$ with orthonormal basis  $\\{\\ket{i}\\}$, and density matrix\n",
    "$$\n",
    "\\rho_B = \\sum_i p_i \\ket{i}\\bra{i}\n",
    "$$\n",
    "and, now, define  the following $AB$-joint  state\n",
    "$$\n",
    "\\rho_{AB} = \\sum_{i} p_i \\rho_i \\otimes \\ket{i}\\bra{i}\n",
    "$$\n",
    "If $\\rho_i $ has eigenvectors  $\\ket{e_{ia}}$ with eigenvalues $\\lambda_{ia}$ so that $\\sum_a \\lambda_{ia} = 1$. Then it is easy to see that  the eigenvalues of $\\rho_{AB}$ will be $\\{ p_i\\lambda_{ia}\\}$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "S_{AB} &=&-  \\sum_{i,a} p_i\\lambda_{ia} \\log( p_i \\lambda_{ia})  =\n",
    "\\sum_i p_i\\log p_i\\sum_a\\lambda_{ia} + \\sum_i p_i \\sum_{a}\\lambda_{ia}\\log\\lambda_{ia} \\nonumber\\\\\n",
    "&=&  \\sum_i p_i\\log p_i+ \\sum_i p_i S(\\rho_i)\\nonumber\\\\\n",
    "&=& S_B + \\sum_i p_i S(\\rho_i) \n",
    "\\end{eqnarray}\n",
    "Taking the partial traces we find\n",
    "\\begin{eqnarray}\n",
    "\\rho_A &=& \\tr_B \\rho_{AB} =  \\sum_{i} p_i \\rho_i \\nonumber\\\\\n",
    "\\rho_B &=& \\tr_A \\rho_{AB} = \\sum_i p_i \\ket{i}\\bra{i} \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now using the subaditivity property\n",
    "$\n",
    "S_{AB} \\leq S_A + S_B\n",
    "$\n",
    "we obtain \n",
    "$$\n",
    "S_{AB} =   S_B + \\sum_i p_i S(\\rho_i)  \\leq S_A  + S_ B=  \\, .\n",
    "$$\n",
    "and cancelling out $S(B)$ we obtain\n",
    "$$\n",
    " \\sum_i p_i S(\\rho_i) \\leq S(A) = S\\left(  \\sum_i p_i\\rho_i \\right)\n",
    "$$\n",
    ":::\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37202c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vamos a ver casos que saturan estas desigualdades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86ffea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Caso 1 : saturación de sub-aditividad: estado  factorizable ##\n",
    "\n",
    "Supongamos que el sistema $AB$ se encuentra en un estado compuesto *factorizable* . Entonces la *información mútua* es nula y, consecuentemente, la *sub-aditividad* se satura\n",
    "\n",
    "$$\n",
    "\\rho_{AB} = \\rho_A \\otimes \\rho_B ~~~~\\Leftrightarrow ~~~~ S(AB) = S(A) + S(B)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab1e4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Caso 2 :  saturación de Araki-Lieb : estado  puro o entrelazado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836fe84",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Comenzaremos por escribir el estado puro \n",
    "\n",
    "$$\n",
    "\\rho_{AB} = \\ket{\\psi_{AB}} \\bra{\\psi_{AB}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En un estado puro de un sistema $AB$ bipartito, el entrelazamiento introduce correlaciones cuánticas entre los dos sistemas\n",
    "\n",
    "$$\n",
    "\\ket{\\psi_{AB}} = \\sum_i c_{ij} \\ket{i}_A\\otimes \\ket{j}_B\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956340",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Una forma de establecer si existe entrelazamiento es usar la descomposición de Schmidt. \n",
    "\n",
    "$$\n",
    "\\ket{\\psi_{AB}} = \\sum_{a=1}^r \\sqrt{p_a}\\,  \\ket{\\psi^a_A}\\otimes \\ket{\\psi^a_B}\n",
    "$$\n",
    "\n",
    "Si y sólo si  el número de Schmidt, $r$, es mayor que uno, el estado está entrelazado. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19af94e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora queremos ser más finos, y proponer una manera de calcular el alcance de dichas correlaciones. Esto es lo que mide la *Entropía de Entrelazamiento*.\n",
    "\n",
    "Efectivamente\n",
    "$$\n",
    "S(\\rho_A) = S(\\rho_B) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n",
    "\n",
    "Por tanto, la *entropía de entrelazamiento* es proporcional al *entrelazamiento* que hay en $\\ket{\\psi_{AB}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ff8f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>:  \n",
    "\n",
    "^^^\n",
    "Sea ${AB}$ un sistema compuesto en un estado puro  $ S(AB) = 0$. La entropía de entrelazamiento de sus subsistemas constituyentes $A$ y $B$ es  \n",
    "\n",
    "    \n",
    "$\\to ~$  igual para ambos subsistemas $S(A) = S(B)$\n",
    "\n",
    "    \n",
    "$\\to ~$  proporcional al entrelazamiento del estado puro $\\ket{\\psi_{AB}}$\n",
    "::::::\n",
    "\n",
    ":::{dropdown} Demostración\n",
    "\n",
    "La matriz densidad $\\rho_{AB} = \\ket{\\psi_{AB}}\\bra{\\psi_{AB}}$ tiene entropía nula $S(\\rho_{AB})=0$. No así las matrices densidad de los subsistemas $A$ y $B$.\n",
    "Escribiendo $\\ket{\\psi_{AB}}$ en la base de Schmidt, podemos calcular\n",
    "\n",
    "\n",
    "$$\n",
    "\\rho_{A} =\\Tr_B \\rho_{AB} =  \\sum_{a} p_a \\ket{\\psi^a_A}\\bra{\\psi^a_A}\n",
    "~~~~~~~,~~~~~~~~\n",
    "\\rho_{B} =\\Tr_A \\rho_{AB} =  \\sum_{a} p_a \\ket{\\psi^a_B}\\bra{\\psi^a_B}\n",
    "$$\n",
    "Como $\\ket{\\psi^a}$ son ortonormales, para las entropías de los subsistemas  encontramos\n",
    "$$\n",
    "S(A) = S(B)  = \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n",
    "\n",
    " - son proporcionales al grado de entrelazamiento de $\\ket{\\psi_{AB}}$. \n",
    "    \n",
    "Efectivamente, si $p_1= 1$ y $p_{i>1} = 0$, con lo que no hay entrelazamiento, entonces la entropía $S(A) = S(B)=0$. \n",
    "    \n",
    "Por el contrario, si el estado es maximalmente mezclado $S = \\log N$, entonces el entrelazamiento también es maximal $p_a = \\frac{1}{\\sqrt{N}}$ para $a = 1,..., N$.\n",
    "\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4156ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vemos que, en este caso, se satura la desigualdad de Araki-Lieb\n",
    "\n",
    "$$\n",
    "|S(A)-S(B)| = 0 = S(AB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc1b30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"codif_optim\"></a>\n",
    "## Codificación cuántica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca15b71",
   "metadata": {},
   "source": [
    "::::::{card} \n",
    "<b>Teorema</b>: </i>de Schuhmacher</i> \n",
    "\n",
    "^^^\n",
    "    \n",
    "Given a message whose letters are pure states drawn independently from the quantum alphabet $X=  \\{ \\ket{\\psi_i}, p_i\\}$,  there exists \n",
    "    an <i>optimal lossless coding</i> that makes an average use of $S(\\rho)$ <i>qubits per letter</i>, where $\\rho = \\sum_i p_i \\ketbra{\\psi_i}{\\psi_i}$.\n",
    "\\end{tcolorbox}\n",
    "\n",
    "::::::\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "414.053px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
